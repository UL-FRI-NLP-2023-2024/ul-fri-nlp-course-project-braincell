Natural Language Inference Set


Gloria Kraker, Ana Kramar, Vanessa Sobočan


University of Ljubljana, Faculty of Computer and Information Science


1. Introduction


Our goal is to create a NLI dataset by creating text passages that challenge the model’s understanding of the following concepts – entailment,
neutrality, and contradiction – between pairs of longer texts. For this purpose, we will use different LLMs, notably ChatGPT3.5, Llama 2.0 and
Gemini, to generate longer texts consisting of two paragraphs by using diverse prompts. We will analyse the accuracy of the chosen models in terms of
following instructions and, if needed, correct the generated texts. The final goal is to train a small model and apply explanation methods to
understand model predictions.


We will research relevant existing work in this area, cited throughout the theoretical part, the complete list of which can be found at the end of
this paper. We will attempt to create a prompt that will in theory be able to get the models to generate sufficient amounts of text while
simultaneously respecting the concepts of entailment and contradiction. If needed, we will tweak the prompt in order to arrive at comparable results
across all LLMs that we intend to use. The models will be prompted through an example sentence, taken from the Brexit corpus, to offer the models a
frame of reference for the subject matter of the text as well as give an example of the level of formality and the kind of linguistic style each
model is expected to match when generating the paragraphs. We have deemed the Brexit corpus as appropriate  for such research because Brexit as a
subject, despite being a politically complex topic, requires the models to include and utilise general areas of knowledge.  From this corpus we will
randomly select 50 sentences to be used on each of the models. Our final dataset will therefore consist of 150 samples of generated text, each
approximately the same length, collected from three separate models, that we will then compile into one dataframe. The samples will be manually
validated by members of our team for entailment, contradiction, and neutrality.


2. Overview of existing resources


The Stanford Natural Language Inference (later SNLI) corpus, proposed by the authors, is a freely available collection of 570K labelled sentence
pairs (expressing the relation of entailment, neutrality or contradiction). The data was gathered via the Amazon Mechanical Turk platform and the
final corpus was written and annotated by humans in a grounded and naturalistic context, with a very high consensus percentage. The SNLI corpus
allows lexicalized classifiers to outperform some sophisticated existing entailment models as well as for neural network-based models to perform
competitively on natural language inference (later NLI) benchmarks for the first time. It respects semantic concepts of entailment and contradiction,
which are central to all aspects of natural language meaning. (Bowman, Angeli, Potts & Manning, 2015)


PLMs are based on transformer architecture and are pre-trained on vast amounts of text data using unsupervised learning techniques like predicting
masked tokens or generating next tokens. BERT, for example, utilises pretraining followed by fine-tuning for downstream tasks, which are further
divided into encoder-only, decoder-only, and encoder-decoder types, and bidirectional and causal based on directivity. (Zhang & Wang, 2023)


Pretrained language models (later PLMs) can be prompted to perform a wide range of language tasks, however the question of how much of this ability
comes from generalizable linguistic understanding versus surface-level lexical patterns remains. To try and answer it, the authors present a
structured prompting approach, allowing them to perform zero- and few-shot sequence tagging with autoregressive PLMs. The evaluation of the model was
then performed by looking at part-of-speech (POS) tagging, named entity recognition (NER) and sentence chunking. Upon conducting the analysis the
authors concluded that PLMs contain significant prior knowledge of task labels. (Blevins, Gonen & Zettlemoyer, 2023)


PLMs have several advantages for Natural Language Reasoning (NLR), including their ability to understand natural language, learn implicit knowledge,
perform in-context learning, and exhibit emergent abilities. PLMs such as BERT and GPT have been essential components in NLP research since their
development and publication. Pre-trained on large-scale text corpora, PLMs are capable of natural language understanding. Researchers theorise that
they might also have the potential to solve reasoning problems. Specifically, PLMs can perform soft deductive reasoning over natural language
statements, reason with implicit knowledge memorised in their parameters, and perform multi-step reasoning step-by-step just with a few
demonstrations or instructions when the model size is large enough via chain-of-thought prompting. ChatGPT and GPT4 also largely contributed to the
development of PLMs with impressive reasoning capabilities. (Zhang & Wang, 2023)


The authors note how empirical developments demonstrate PLMs' potential for natural language reasoning. Such models are also able to perform
defeasible reasoning. PLMs can produce faithful reasoning paths, marking them as promising for processing various other aspects of natural language,
commonsense knowledge, and defeasible reasoning. Whereas Large Language Models (later LLMs) have shown capabilities with techniques like Chain-of
Thought prompting, forward reasoning paths, and question decomposition. PLMs are large enough for generating a reasoning path before the final answer
which can significantly improve the multi-step reasoning performance. This fact alone has inspired much research in this area. Because of the
limitations of forward reasoning, much research has been focused in the opposite direction. Backward reasoning is more efficient than forward
reasoning due to the smaller search space. While forward reasoning can expose arbitrary new knowledge entailed by the existing one, backward
reasoning just targets the specific goal or the problem solution. A typical approach of backward reasoning is question decomposition, which can
improve performance on multi-hop questions for both medium-size PLMs and LLMs. (Zhang & Wang, 2023)


3. Experimental setup


3.1. Models


We observe the performance of three language models, namely ChatGPT 4, Llama 2 and Gemini.


3.1.1. ChatGPT 3.5


3.1.2. Lllama 2


3.1.3. Gemini




3.2. Prompt formatting


Our initial challenge was the layout of the prompt itself, as we quickly concluded that even small modifications yield different results. We tested
diverse formats and in the end, opted for a very structured prompt that clearly states first the general instructions and then further divides those
in an attempt to make the models remember and follow all of the demands.


In the semantic wording for our prompt, we tried to include the following information in a single phrase: who the model is writing as, what the
objective is and what we expected as a result – in sense of format (two paragraphs of approximately 5 sentences in length) and content (in what
manner the model would have to express either the entailment or contradiction relations). In our case specifically, this means the model is writing
as itself and the task is generating sentences to aid in the creation of a NLI dataset for the understanding of entailment, neutrality and
contradiction in natural language processing. In the structure of the prompt, we specified what the instructions will look like by providing an
example, including parts such as “insert statement here” to denote the space where we would write our various inputs. At the end, we made sure to
include “end of example” to ensure that the models would clearly distinguish between the instructions and the example.


In our prompt, we decided to separate the instructions regarding the format and the objective, to increase the likelihood of the model providing
sufficient results. This allows us to also occasionally repeat our instructions and to make it more likely for the models to carry them out. The
statement “The provided statements will be of political nature, especially regarding Brexit. Please ensure you have sufficient knowledge of the
subject.” was added later on in the prompt engineering process in order to check whether the model in question has the required knowledge. At the end
of the prompt we specify that the model needs to remember our prompt for all the tasks and should implement it consistently, as well as ask the model
to confirm that it saved the instructions properly, so that it can access them later on.


4. Results


4.1. Overall results




4.1.1. ChatGPT 3.5


4.1.2. Llama 2


4.1.3. Gemini




4.2. Error analysis




5. Related work


5.1. A large annotated corpus for learning natural language inference


The SNLI corpus can be used to evaluate a variety of models for NLI, from rule-based systems, simple linear classifiers to neural network-based
models. Their conclusion being that both a feature-rich classifier model as well as a neural network model centred around a Long Short-Term Memory
(LSTM) network achieve comparable performance, the latter with the support for transfer learning, which can be adapted to an existing NLI challenge
task. A large sized corpus is crucial to both LTSM and the lexicalized model, suggesting that additional data would improve performance in both.
Further work on compositional semantics is also needed. Some issues also arise from inferences depending on world knowledge and context-specific
inferences as models sometimes try to take shortcuts by relying solely on lexical cues. (Bowman, Angeli, Potts & Manning, 2015)


5.2. Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches
Textual entailment tasks. A highly popular format was originally introduced by the RTE Challenges, where given a pair of texts, i.e., a context and
hypothesis, one must determine whether the context entails the hypothesis (Dagan et al., 2005). In the fourth and fifth RTE Challenges, this format
was extended to a three-way decision problem where the hypothesis may contradict the context (Giampiccolo et al., 2008; Bentivogli et al., 2009).
While this format is typically used for textual entailment problems like the RTE Challenges, it can also be used for nearly any type of inference
problem. (Storks, Gao, Chai, 2020)
5.3. Nature language reasoning, a survey
The authors propose three definitions of reasoning, namely natural language reasoning, negation-based reasoning and task-based reasoning. This study
focuses on the single-modality unstructured natural language text (without knowledge triples, tables and intermediate formal language) and natural
language reasoning (rather than symbolic reasoning and mathematical reasoning). It is more important for NLP models to perform interpretable
defeasible reasoning, seeing as people with different background knowledge can infer very different and sometimes even opposite conclusions by
themselves, thus it is much more difficult to clarify the conclusion without explicit premises and a clearly laid out reasoning procedure.
Interpretability and faithful reasoning. Transparent and reliable reasoning paths become increasingly important when it generalises to longer steps
and defeasible reasoning. Firstly, when there are many steps, it takes more time and effort for people to check the quality of reasoning. Therefore,
unfaithful reasoning might introduce difficulty in people’s judgement and decision-making. Secondly, when it comes to defeasible reasoning, exposing
interpretable reasoning paths is much more important and sometimes necessary for people to be convinced. In this case, different people with
different background knowledge can derive different and even opposed conclusions, thus it is crucial to illustrate the evidence collected to reason.
(Zhang & Wang, 2023)


6. Conclusion


6.1. Limitations and Future work


Superficial correlation bias. The kind of biases most difficult to discern and avoid perhaps are those caused by accidental correlations between
features of answers and questions. One example of this is gender bias, which NLI systems are particularly vulnerable to when trained on biased data.
Rudinger et al. (2018a) highlight this problem in coreference resolution, showing that systems trained on gender-biased data perform worse in gender
pronoun disambiguation. For example, consider the problem from their Winogender dataset: "The paramedic performed CPR on the passenger even though
she knew it was too late." In determining who she is, systems trained on gender-biased training data may be more likely, for example, to incorrectly
choose the passenger rather than the paramedic due to male gender pronouns appearing in training data more commonly in the context of this occupation
than female gender pronouns. To avoid this, gender pronouns should appear equally frequently among other words in training data, especially those
related to occupations and activities. (Storks, Gao, Chai, 2020)


Information-retrieval, semantic parsing and commonsense reasoning tasks. The authors acknowledge that the existing NLI corpora lack in size for the
purposes of training modern data-intensive, wide-coverage models. Many lexical relationships are still misanalyzed, leading to incorrect predictions.
Furthermore, the indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label. Using
vector representations of sentence pairs, each sentence is embedded and evaluated at every level by means of a neural network classifier. (Bowman,
Angeli, Potts & Manning, 2015)
Generalisation to longer steps. The multi-step performance degrades as PLMs encounter samples that require more reasoning steps than those in
training data or few-shot exemplars. (Zhang & Wang, 2023)
Reasoning over non-English languages. In addition to reason over English statements, it is also important to perform reasoning training tasks with
other languages, which often prove to be much more challenging due to more apparent data sparsity problems. While specialised models were designed to
improve evidence aggregation, reasoning capability or faithfulness, they are nonetheless constrained to specific tasks, datasets or reasoning types
that do not perform well with  the generalisation. Since transformers have been found to be soft deductive reasoners after in-domain finetuning,
vanilla PLMs have been more popular to perform reasoning. However, data sparsity and spurious correlation problems make it difficult for medium-size
PLMs to learn the general logical structure of diverse reasoning types. While there is much research on deductive reasoning, defeasible reasoning is
much more challenging for PLMs and is still under-explored. (Zhang & Wang, 2023)


7. Keywords
Large language model, prompt, entailment, neutrality, contradiction


8. References


* Klemen, M., Žagar, A., Čibej, J. & Robnik-Šikonja, M., 2022. Slovene Natural Language Inference Dataset SI-NLI. Slovenian language resource 
repository CLARIN.SI, ISSN 2820-4042.  


* Yu, F., Zhang, H., & Wang, B. (2023). Nature language reasoning, a survey. arXiv preprint arXiv:2303.14725.


* Bowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus for learning natural language inference. arXiv preprint
arXiv:1508.05326. 


* Shane Storks, Qiaozi Gao, Joyce Y. Chai. Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches. (2020). 


* Blevins, T., Gonen, H., Zettlemoyer, L.  Prompting Language Models for Linguistic Structure.  (2022). 


9. Appendix


9.1. Prompt outline
You, as an LLM, are tasked with generating text passages to aid in the creation of a NLI data set for the understanding of entailment, neutrality,
and contradiction in natural language processing.
For each task, a statement will be provided in the space marked {{INSERT STATEMENT HERE}}. Based on this statement, you are to write a text
consisting of two paragraphs. Each paragraph must be approximately five sentences long. The paragraphs must form a single uniform unit, yet they must
be inter-connected via entailment, contradiction or neutrality.
The provided statements will be of political nature, especially regarding Brexit. Please ensure you have sufficient knowledge of the subject.
Objective: The objective is to create text passages that have a clear and distinct relationship with the provided statement. In the text passages,
the inference relationships are categorised into entailment, contradiction, or neutrality. It is crucial that the nature of the relationship
(entailment, contradiction, neutrality) is evident through the context and content of the paragraphs but is not directly stated or labelled within
the text.
Paragraph Requirements:
A paragraph should either logically follow from the statement, providing a scenario or context where the statement is clearly true, or present a
scenario or context that logically contradicts the statement, demonstrating a situation where the statement would be false.
Guidelines:
The text must consist of two paragraphs which are connected via entailment, contradiction, or neutrality. The paragraphs must be separated by a blank
line to maintain clear structure.
The paragraphs should be approximately five sentences each, ensuring sufficient detail and context.
Do not explicitly mention which type of relationship (entailment, contradiction, neutrality) the paragraphs are representing. The relationship should
be inferred from the content.
Example Task:
{{INSERT STATEMENT HERE}}
Your task: Based on the statement provided above, write two paragraphs as described, focusing on crafting scenarios that have a clear and distinct
relationship with the provided statement. Do not forget that the paragraphs you write must be internally connected and reflect entailment and/or
contradiction relationships.
END OF EXAMPLE
Please store these instructions in your knowledge base. If there are any similar pre-existing instructions, overwrite them.
Before we begin, please confirm that you understand these instructions and that they are properly stored in your knowledge base.
